{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a093d0f8a7d2d4e2",
   "metadata": {},
   "source": [
    "# 10. 임베딩 모델로 데이터 의미 압축하기\n",
    "\n",
    "1. 텍스트를 숫자로 표현하려던 다양한 시도를 살펴본다.\n",
    "2. 임베딩 벡터의 유사도를 기반으로 검색하는 방법(의미 검색)을 실습\n",
    "3. 의미 검색의 단점을 보완하기 위해 키워드 검색을 조합해 사용하는 하이브리드 검색을 실습\n",
    "\n",
    "### Reference\n",
    "- [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)"
   ]
  },
  {
   "cell_type": "code",
   "id": "25c605e226f82957",
   "metadata": {},
   "source": [
    "!pip install transformers datasets sentence-transformers faiss-cpu llama-index llama-index-embeddings-huggingface -q\n",
    "\n",
    "# -q: quiet mode (출력 메시지를 최소화합니다.)\n",
    "# -qq : more quiet mode (경고 메시지만 출력합니다.)\n",
    "# -qqq: even more quiet mode (어떤 메시지도 출력하지 않습니다.)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 10.1 텍스트 임베딩 이해하기\n",
    "\n",
    "## 10.1.1 문장 임베딩 방식의 장점\n",
    "\n",
    "- 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식을 `텍스트 임베딩` 또는 `문장 임베딩`이라 부른다.\n",
    "- `임베딩`이란 \"데이터의 의미를 압축한 숫자 배열(벡터)\"를 말한다.\n",
    "- `임베딩`을 통해 데이터가 서로 유사한지, 관련이 있는지와 같이 중요한 정보를 활용할 수 있다."
   ],
   "id": "8f3530c10718ab12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 예제 10.1 문장 임베딩을 활용한 단어 간 유사도 계산\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "smodel = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')  # https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
    "smodel # 약 10초 소요"
   ],
   "id": "eabe6ef21ac9171d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dense_embeddings = smodel.encode(['학교', '공부', '운동'])\n",
    "cosine_similarity(dense_embeddings)"
   ],
   "id": "38276b25ffa15d78",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f795b2b4de2b8454",
   "metadata": {},
   "source": [
    "### 유사도\n",
    "- 유클리드 거리: 두 벡터 사이의 직선 거리를 계산\n",
    "\n",
    "    ![image.png](resources/euclidean.png)\n",
    "- 코사인 유사도: 두 벡터 사이의 각도를 기반으로 유사도를 측정 (0과 1 사이의 값)\n",
    "\n",
    "    ![image.png](resources/cosine.png)\n",
    "\n",
    "```\n",
    "GPT:\n",
    "유클리드 거리는 벡터의 \"크기\"와 \"방향\" 모두를 고려합니다.\n",
    "반면 코사인 유사도는 벡터의 \"방향\"만을 비교하고 \"크기\"는 무시합니다.\n",
    "\n",
    "텍스트 임베딩에서는 벡터의 크기가 문장 길이나 빈도 등 불필요한 정보에 영향을 받을 수 있습니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824fb424ea2bee8",
   "metadata": {},
   "source": [
    "## 10.1.2 원핫 인코딩\n",
    "\n",
    "- 학교, 공부, 운동을 예시로 아래와 같이 표현하는 것을 원핫 인코딩이라고 한다.\n",
    "<table style=\"width: 50%\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"text-align: center\">단어</th>\n",
    "            <th style=\"text-align: center\"></th>\n",
    "            <th style=\"text-align: center\"></th>\n",
    "            <th style=\"text-align: center\"></th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">학교</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">공부</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">운동</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "- 이 방식을 사용하면 '식사'라는 새로운 데이터를 추가해도 아래처럼 독립적으로 추가할 수 있고, 단어와 단어 사이에 아무런 관계도 나타내지 않는다.\n",
    "<table style=\"width: 50%\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"text-align: center\">단어</th>\n",
    "            <th style=\"text-align: center\"></th>\n",
    "            <th style=\"text-align: center\"></th>\n",
    "            <th style=\"text-align: center\"></th>\n",
    "            <th style=\"text-align: center\"></th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">학교</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">공부</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">운동</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">식사</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "### 장점\n",
    "- 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점\n",
    "\n",
    "### 단점\n",
    "- 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 단점"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8287f6c5e36dd39",
   "metadata": {},
   "source": [
    "# 예제 10.2 원핫 인코딩의 한계\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "word_dict = {\n",
    "    \"school\": np.array([[1, 0, 0]]),\n",
    "    \"study\": np.array([[0, 1, 0]]),\n",
    "    \"workout\": np.array([[0, 1, 0]]),\n",
    "}\n",
    "cosine_similarity(word_dict['school'], word_dict['study'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27cae94616cffc5a",
   "metadata": {},
   "source": [
    "cosine_similarity(word_dict['school'], word_dict['workout'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5114c279c457a89",
   "metadata": {},
   "source": [
    "## 10.1.2 백오브워즈\n",
    "\n",
    "- 백오브워즈(Bag Of Words)는 '비슷한 단어가 많이 나오면 비슷한 문장 또는 문서'라는 가정을 활용해 문서를 숫자로 변환한다.\n",
    "\n",
    "<table style=\"width: 50%\">\n",
    "<thead>\n",
    "    <tr>\n",
    "        <th style=\"text-align: center\"></th>\n",
    "        <th style=\"text-align: center\">가계 대출</th>\n",
    "        <th style=\"text-align: center\">증시</th>\n",
    "        <th style=\"text-align: center\">AI</th>\n",
    "        <th style=\"text-align: center\">부동산</th>\n",
    "        <th style=\"text-align: center\">LLM</th>\n",
    "        <th style=\"text-align: center\">구글</th>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">경제 기사 1</td>\n",
    "            <td style=\"text-align: center\">3</td>\n",
    "            <td style=\"text-align: center\">3</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">2</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">경제 기사 2</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">5</td>\n",
    "            <td style=\"text-align: center\">3</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">IT 기사 2</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">3</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">4</td>\n",
    "            <td style=\"text-align: center\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">IT 기사 2</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "            <td style=\"text-align: center\">2</td>\n",
    "            <td style=\"text-align: center\">1</td>\n",
    "            <td style=\"text-align: center\">2</td>\n",
    "            <td style=\"text-align: center\">0</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "### 장점\n",
    "- 아이디어가 직관적이고 구현이 간단함에도 훌륭히(?) 작동하기 때문에 문장과 문서의 의미를 표현하는 방법으로 오랫동안 사용\n",
    "### 단점\n",
    "- 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는데 크게 도움이 되지 않는 경우가 있다는 단점\n",
    "    - 예를 들어, 조사('은/는/이/가', '을/를')는 거의 모든 한국어 문서에 등장 (불용어 전처리 필요)\n",
    "- `AI`라는 단어는 여러 기사에서 언급하기 때문에 `AI`라는 단어가 등장했다는 사실만으로는 문서의 의미를 예측하기 어려움"
   ]
  },
  {
   "cell_type": "code",
   "id": "242d68d9da97908e",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['want to go home home', 'want to go work work']\n",
    "vector = CountVectorizer()\n",
    "transform = vector.fit_transform(corpus)\n",
    "\n",
    "sorted(vector.vocabulary_.items(), key=lambda x: x[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed4e82b4853064e7",
   "metadata": {},
   "source": [
    "transform.toarray()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b57108af30392bb5",
   "metadata": {},
   "source": [
    "## 10.1.3 TF-IDF\n",
    "- TF-IDF(Term Frequency-Inverse Document Frequency)는 백오브워즈의 단점을 보완하기 위해 등장한 방법\n",
    "$$ \\mathrm {TFIDF} (w)=\\mathrm{TF}(w) \\times \\log(N / \\mathrm{DF}(w)) $$\n",
    "- TF(w): 문서에서 단어 w의 빈도\n",
    "- DF(w): 단어 w가 등장한 문서의 수\n",
    "- N: 전체 문서의 수\n",
    "\n",
    "| | TF(\"이\") | TF(\"LLM\") | TF(\"AI\") | DF(\"이\") | DF(\"LLM\") | DF(\"AI\") | TF-IDF(\"이\") | TF-IDF(\"LLM\") | TF-IDF(\"AI\") |\n",
    "|---|---------|-----------|----------|---------|-----------|----------|-------------|--------------|--------------|\n",
    "| 경제 기사 1 | 10      | 0       | 0        | 4       | 2         | 3        | 0           | 0            | 0            |\n",
    "| 경제 기사 2 | 8       | 0       | 3        | 4       | 2       | 3        | 0           | 0            | 3 * log(4/3) |\n",
    "| IT 기사 1 | 5       | 4       | 3        | 4       | 2       | 3        | 0           | 4 * log(4/2) | 3 * log(4/3) |\n",
    "| IT 기사 2 | 9       | 2         | 2        |  4      | 2       | 3        | 0           | 2 * log(4/2) | 2 * log(4/3) |\n",
    "\n",
    "- 조사 '이'는 모든 문서에 등장하기 때문에 TF-IDF(\"이\")는 0이 된다. (중요도가 없다.)\n",
    "- 백오브워즈의 문제를 성공적으로 보완하면서 오랫동안 활발히 사용\n",
    "    - 이후에 설명하는 BM25(Best Matching 25)와 같은 TF-IDF의 변형 방식이 현재까지도 가장 보편적인 연관도 점수 계산 방식으로 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ab9b2ec74dc54b2",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "corpus = ['want to go home home', 'want to go work work', 'hope to go home home', 'hope to go work work']\n",
    "vector = TfidfVectorizer(norm=None)\n",
    "transform = vector.fit_transform(corpus)\n",
    "\n",
    "sorted(vector.vocabulary_.items(), key=lambda x: x[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2341ec254afbfa",
   "metadata": {},
   "source": [
    "vector.idf_ # sklearn의 TfidfVectorizer는 idf 값에 +1을 더한 값을 반환합니다."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acce880ac3bf1ed4",
   "metadata": {},
   "source": [
    "transform.toarray()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf0958734c484d5e",
   "metadata": {},
   "source": [
    "## 10.1.4 워드투벡\n",
    "- 워드투벡(Word2Vec)은 단어가 `함께 등장하는 빈도` 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법\n",
    "  - \"AI\"는 \"ML\" 또는 \"머신러닝\"\n",
    "  - \"한강\"은 \"라면\"이나 \"자전거\"\n",
    "\n",
    "---\n",
    "\n",
    "- 특정 단어 주변에 어떤 단어가 있는지 예측하는 모델을 만든다면 단어의 의미를 표현한 임베딩을 모델(인공신경망)이 생성할 수 있지 않을까 하는 가정\n",
    "  - 주변 단어로 중간 단어를 예측하는 방식(CBOW, Continuous Bagof Words)\n",
    "  - 중간 단어로 주변 단어를 예측하는 방식(Skip-Gram)\n",
    "\n",
    "![image.png](resources/Word2Vec-architecture.png)\n",
    "\n",
    "- 학습 전 벡터는 원핫 인코딩 방식으로 초기화 합니다.\n",
    "\n",
    "![image.png](resources/Word2Vec-data.png)\n",
    "\n",
    "---\n",
    "\n",
    "- CBOW는 주변의 단어 정보로 중간에 있을 단어를 예측하는 방식\n",
    "    - t번째 단어를 예측하기 위해 위아래로 2개의 단어 정보를 활용 (t-2, t-1, t+1, t+2)\n",
    "\n",
    "![image.png](resources/Word2Vec-cbow.png)\n",
    "![image.png](resources/Word2Vec-cbow2.png)\n",
    "\n",
    "---\n",
    "\n",
    "- Skip-Gram은 중간 단어로 주변 단어를 예측하는 방식\n",
    "    - t번째 단어를 중심으로 위아래로 2개의 단어를 예측 (t-2, t-1, t+1, t+2)\n",
    "\n",
    "![image.png](resources/Word2Vec-skipgram.png)\n",
    "![image.png](resources/Word2Vec-skipgram2.png)\n",
    "\n",
    "- 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-Gram 방식이 CBOW보다 성능이 좋다고 알려져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9a378103f3fab",
   "metadata": {},
   "source": [
    "# 10.2 문장 임베딩 방식\n",
    "- 텍스트를 활용할 때 단어 단위보다 문장, 문단 같은 더 큰 단위를 사용\n",
    "- 여러 단어가 합쳐진 문장을 임베딩 벡터로 변환하는 방법이 필요\n",
    "\n",
    "## 10.2.1 문장 사이의 관계를 계산하는 두 가지 방법\n",
    "- BERT(Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 기반 모델은 문장 임베딩을 계산하는 데 사용되는 대표적인 모델\n",
    "\n",
    "![image.png](resources/sentence-encoder.png)\n",
    "\n",
    "### 바이 인코더(bi-encoder)\n",
    "- 각각의 문장을 독립적으로 BERT 모델에 입력 (문장 A, B)\n",
    "- 풀링 층은 문장의 길이가 달라져도 문장 임베딩의 차원이 같도록 맞춰주는 층\n",
    "- 출력 결과인 문장 임베딩 벡터(u, v)를 계산\n",
    "- 코사인 유사도와 같은 별도의 계산 방식을 통해 유사도 계산\n",
    "\n",
    "### 교차 인코더(cross-encoder)\n",
    "- 문장 A와 B를 함께 입력\n",
    "- 출력결과 자체가 유사도\n",
    "- 바이 인코더 방식에 비해 계산량이 많지만, 두 문장의 상호작용을 고려할 수 있어 좀 더 정확한 관계 예측이 가능\n",
    "- 하지만 입력으로 넣은 두 문장의 유사도만 계산하기 때문에 다른 문장과 검색 쿼리의 유사도를 알고 싶으면 다시 동일한 과정을 반복 (p.339~341)\n",
    "\n",
    "## 10.2.2 바이 인코더 모델 구조\n",
    "- BERT 모델의 출력을 풀링 층을 통해 고정된 크기의 문장 임베딩으로 만든다.\n",
    "    - BERT 모델은 입력 토큰마다 출력 임베딩을 생성\n",
    "    - 입력하는 문장의 길이가 달라질 경우, 출력하는 임베딩의 수가 달라진다.\n",
    "    - position encoding + self-attention을 통해 문맥을 반영\n",
    "![image.png](resources/bi-encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "32696e30609cf9aa",
   "metadata": {},
   "source": [
    "# 예제 10.3 Sentence-Transformers 라이브러리로 바이 인코더 생성하기\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "# 사용할 BERT 모델\n",
    "word_embedding_model = models.Transformer('klue/roberta-base')\n",
    "# word_embedding_model = models.Transformer('klue/bert-base') # token_type_ids 구분 모델\n",
    "\n",
    "# 풀링 층 차원 입력\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 두 모듈 결합하기\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75301e1ef02344d3",
   "metadata": {},
   "source": [
    "model의 출력을 통해 바이 인코더의 구조를 확인할 수 있다.\n",
    "- `Transformer.max_seq_length` 입력 문장의 최대 길이 (토큰의 수)\n",
    "- `Pooling.word_embedding_dimension` BERT 모델의 출력 임베딩 차원\n",
    "- `Pooling.pooling_mode_...` 풀링 층의 모드\n",
    "\n",
    "```\n",
    "GPT\n",
    "\n",
    "Pooling 층에서 pooling_mode_cls_token, pooling_mode_mean_tokens, pooling_mode_max_tokens를 모두 True로 설정하면,\n",
    "각각의 방식([CLS] 토큰, 평균, 최대값)으로 임베딩을 모두 계산해서 하나의 벡터로 이어붙입니다(concatenate).\n",
    "```\n",
    "\n",
    "### Pooling mode\n",
    "- 클래스 모드 (pooling_mode_cls_tokens)\n",
    "  - [CLS] 토큰을 사용하여 문장 임베딩을 생성"
   ]
  },
  {
   "cell_type": "code",
   "id": "302256bdedab4e71",
   "metadata": {},
   "source": [
    "test_model = SentenceTransformer(modules=[word_embedding_model])\n",
    "test_model.tokenize([['잠이 안 옵니다', '졸음이 옵니다']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_model.encode('잠이 안 옵니다', output_value='token_embeddings')",
   "id": "ce6706f9bec2fba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ffcc5b9b8868ac1",
   "metadata": {},
   "source": [
    "# 예제 10.3 - 1 클래스 모드\n",
    "import torch\n",
    "\n",
    "\n",
    "def class_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    [CLS] 토큰을 사용한 클래스 모드\n",
    "    :param model_output: 언어 모델의 출력\n",
    "    :param attention_mask: 패딩 토큰의 위치를 확인할 수 있는 어텐션 마스크\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    token_embeddings = model_output[0]  # 언어 모델의 출력 중 마지막 층의 출력만 사용\n",
    "    cls_embeddings = token_embeddings[:, 0, :]  # [CLS] 토큰의 임베딩을 가져온다.\n",
    "    return cls_embeddings  # [CLS] 토큰의 임베딩을 반환한다.\n",
    "\n",
    "\n",
    "# 테스트\n",
    "sentences = [\n",
    "    \"잠이 안 옵니다\",\n",
    "]\n",
    "\n",
    "model_output = (\n",
    "    torch.tensor([\n",
    "        [[0.15,-0.62], [-0.63,0.07], [0.17,-0.62], [0.47,-0.34], [0.49,0.00], [0.15,-0.62]],  # \"[CLS] 잠이 안 옵니다\"\n",
    "    ]),\n",
    ")\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 1, 1, 1],  # \"[CLS] 잠이 안 옵니다\"\n",
    "])\n",
    "\n",
    "class_pooling(model_output, attention_mask)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_model.tokenizer",
   "id": "fea1b67a7f6cacca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43900c74add8e4be",
   "metadata": {},
   "source": [
    "- 평균 모드 (pooling_mode_mean_tokens)\n",
    "  - 모든 토큰의 임베딩을 평균내어 문장 임베딩을 생성"
   ]
  },
  {
   "cell_type": "code",
   "id": "19041de68a12a0f1",
   "metadata": {},
   "source": [
    "# 예제 10.4 평균 모드\n",
    "import torch\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    평균 모드\n",
    "    :param model_output: 언어 모델의 출력\n",
    "    :param attention_mask: 패딩 토큰의 위치를 확인할 수 있는 어텐션 마스크\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    token_embeddings = model_output[0]  # 언어 모델의 출력 중 마지막 층의 출력만 사용\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()  # 입력이 패딩 토큰인 부분은 평균 계산에서 무시하기 위해 input_mask_expanded를 만들고\n",
    "\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)  # input_mask_expanded를 출력 임베딩에 곱해 준다.\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)  # 마지막으로 출력 임베딩의 합을 패딩 토큰이 아닌 실제 토큰 입력의 수로 나눠준다.\n",
    "    return sum_embeddings / sum_mask  # 평균 계산\n",
    "\n",
    "\n",
    "# 테스트\n",
    "sentences = [\n",
    "    \"잠이 안 옵니다\",\n",
    "]\n",
    "\n",
    "model_output = (\n",
    "    torch.tensor([\n",
    "        [[0.15,-0.62], [-0.63,0.07], [0.17,-0.62], [0.47,-0.34], [0.49,0.00], [0.15,-0.62]],  # \"[CLS] 잠이 안 옵니다\"\n",
    "    ]),\n",
    ")\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 1, 1, 1],  # \"[CLS] 잠이 안 옵니다\"\n",
    "])\n",
    "\n",
    "mean_pooling(model_output, attention_mask)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16ea7810eee2c633",
   "metadata": {},
   "source": [
    "- 최대 모드 (pooling_mode_max_tokens)\n",
    "  - 모든 토큰의 임베딩 중 최대값을 사용하여 문장 임베딩을 생성"
   ]
  },
  {
   "cell_type": "code",
   "id": "55ad349a02f8e3dd",
   "metadata": {},
   "source": [
    "# 예제 10.5 최대 모드\n",
    "def max_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    최대 모드\n",
    "    :param model_output: 언어 모델의 출력\n",
    "    :param attention_mask: 패딩 토큰의 위치를 확인할 수 있는 어텐션 마스크\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    token_embeddings = model_output[0]  # 언어 모델의 출력 중 마지막 층의 출력만 사용\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()  # 입력이 패딩 토큰인 부분은 평균 계산에서 무시하기 위해 input_mask_expanded를 만들고\n",
    "    token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "    return torch.max(token_embeddings, 1)[0]\n",
    "\n",
    "# 테스트\n",
    "sentences = [\n",
    "    \"잠이 안 옵니다\",\n",
    "]\n",
    "\n",
    "model_output = (\n",
    "    torch.tensor([\n",
    "        [[0.15,-0.62], [-0.63,0.07], [0.17,-0.62], [0.47,-0.34], [0.49,0.00], [0.15,-0.62]],  # \"[CLS] 잠이 안 옵니다\"\n",
    "    ]),\n",
    ")\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 1, 1, 1],  # \"[CLS] 잠이 안 옵니다\"\n",
    "])\n",
    "\n",
    "max_pooling(model_output, attention_mask)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfadb5b629d19a7e",
   "metadata": {},
   "source": [
    "## 10.2.3 Sentence-Transformers로 텍스트와 이미지 임베딩 생성해 보기\n",
    "\n",
    "### 문장 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "id": "c34aa9a6de83521b",
   "metadata": {},
   "source": [
    "# 예제 10.6 Sentence-Transformers로 텍스트와 이미지 임베딩 생성해 보기\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('snunlp/KR-SBERT-V40k-klueNLI-augSTS')\n",
    "\n",
    "embs = model.encode(['잠이 안 옵니다', '졸음이 옵니다', '기차가 옵니다'])\n",
    "\n",
    "cos_scores = util.cos_sim(embs, embs)\n",
    "cos_scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 이미지 임베딩",
   "id": "51195db27df7db0d"
  },
  {
   "cell_type": "code",
   "id": "4a7908ca69221a5f",
   "metadata": {},
   "source": [
    "# 예제 10.7 CLIP 모델을 활용한 이미지와 텍스트 임베딩 유사도 계산\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "img_embs = model.encode([Image.open('resources/dog.jpg'), Image.open('resources/cat.jpg')])\n",
    "text_embs = model.encode(['A dog on grass', 'Brown cat on yellow background'])\n",
    "\n",
    "cos_scores = util.cos_sim(img_embs, text_embs)\n",
    "cos_scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "133c74dda0f38ebf",
   "metadata": {},
   "source": [
    "## 10.2.4 오픈소스와 상업용 임베딩 모델 비교하기\n",
    "\n",
    "- https://platform.openai.com/docs/guides/embeddings\n",
    "    - text-embedding-ada-002\n",
    "    - text-embedding-3-small\n",
    "    - text-embedding-3-large"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
