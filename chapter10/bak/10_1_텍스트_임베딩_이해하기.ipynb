{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcae70cc4f730392",
   "metadata": {},
   "source": [
    "# 10.1 텍스트 임베딩 이해하기\n",
    "\n",
    "- 문장을 벡터로 변환하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fb0d5f78e7881",
   "metadata": {},
   "source": [
    "## 10.1.1 문장 임베딩 방식의 장점\n",
    "\n",
    "- 데이터의 의미를 숫자로 표현할 수 있다면, 데이터가 서로 유사한지, 관련이 있는지와 같이 중요한 정보를 활용할 수 있다.\n",
    "- 이번 예제에서는 문장 임베딩 방식을 사용하면 단어나 문장 사이의 관계를 계산할 수 있다는 점만 이해해도 충분하다.\n",
    "- 원핫 인코딩, 백오브워즈, TF-IDF, 워드투벡을 순서대로 알아본다."
   ]
  },
  {
   "cell_type": "code",
   "id": "f080f2c1-15ac-4dba-a1ae-624b2ec72d57",
   "metadata": {},
   "source": [
    "# 예제 10.1 문장 임베딩을 활용한 단어 간 유사도 계산\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "smodel = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')  # https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
    "smodel # 일반적으로 work_embedding_dimension은 주로 768, 1024 차원으로 설정된다. (차원은 의미를 담을 수 있는 공간), 256의 배수로 표현되는 것 같은데 이유는 모르겠음"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fcb1d3e1025ba1",
   "metadata": {},
   "source": [
    "dense_embeddings = smodel.encode(['학교', '공부', '운동'])\n",
    "cosine_similarity(dense_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c384d5da95cefe46",
   "metadata": {},
   "source": [
    "## 10.1.2 원핫 인코딩\n",
    "\n",
    "- 학교, 공부, 운동을 예시로 아래와 같이 표현하는 것을 원핫 인코딩이라고 한다.\n",
    "\n",
    "| 단어 |  |  |  |\n",
    "|---|--|---|---|\n",
    "| 학교 | 1 | 0 | 0 |\n",
    "| 공부 | 0 | 1 | 0 |\n",
    "| 운동 | 0 | 0 | 1 |\n",
    "\n",
    "- 이 방식을 사용하면 '식사'라는 새로운 데이터를 추가해도 아래처럼 독립적으로 추가할 수 있고, 단어와 단어 사이에 아무런 관계도 나타내지 않는다.\n",
    "\n",
    "| 단어 |  |  |  |  |\n",
    "|---|--|---|---|---|\n",
    "| 학교 | 1 | 0 | 0 | 0 |\n",
    "| 공부 | 0 | 1 | 0 | 0 |\n",
    "| 운동 | 0 | 0 | 1 | 0 |\n",
    "| 식사 | 0 | 0 | 0 | 1 |\n",
    "\n",
    "### 장점\n",
    "- 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점\n",
    "\n",
    "### 단점\n",
    "- 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 단점"
   ]
  },
  {
   "cell_type": "code",
   "id": "2989d1b40b6b8a44",
   "metadata": {},
   "source": [
    "# 예제 10.2 원핫 인코딩의 한계\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "word_dict = {\n",
    "    \"school\": np.array([[1, 0, 0]]),\n",
    "    \"study\": np.array([[0, 1, 0]]),\n",
    "    \"workout\": np.array([[0, 1, 0]]),\n",
    "}\n",
    "cosine_similarity(word_dict['school'], word_dict['study'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de88adeca69837d8",
   "metadata": {},
   "source": [
    "cosine_similarity(word_dict['school'], word_dict['workout'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da1b7ddec00d9cb5",
   "metadata": {},
   "source": [
    "## 10.1.2 백오브워즈\n",
    "\n",
    "- 백오브워즈(Bag Of Words)는 '비슷한 단어가 많이 나오면 비슷한 문장 또는 문서'라는 가정을 활용해 문서를 숫자로 변환한다.\n",
    "\n",
    "| | 가계 대출 | 증시 | AI | 부동산 | LLM | 구글 |\n",
    "|---|-------|----|----|-----|-----|----|\n",
    "| 경제 기사 1 | 3     | 3  | 0  | 2   | 0   | 0  |\n",
    "| 경제 기사 2 | 0     | 5  | 3  | 1   | 0   | 0  |\n",
    "| IT 기사 1 | 0     | 0  | 3  | 0   | 4   | 2  |\n",
    "| IT 기사 2 | 0     | 0  | 2  | 1   | 2   | 0  |\n",
    "\n",
    "### 장점\n",
    "- 아이디어가 직관적이고 구현이 간단함에도 훌륭히 작동하기 때문에 문장과 문서의 의미를 표현하는 방법으로 오랫동안 사용\n",
    "### 단점\n",
    "- 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는데 크게 도움이 되지 않는 경우가 있다는 단점\n",
    "    - 예를 들어, 조사('은/는/이/가', '을/를')는 거의 모든 한국어 문서에 등장 (불용어 전처리 필요)\n",
    "- `AI`라는 단어는 여러 기사에서 언급하기 때문에 `AI`라는 단어가 등장했다는 사실만으로는 문서의 의미를 예측하기 어려움"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f8df4f69b7ddcc",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['i want to go home', 'i want to go work']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print('BOW Vector', vector.fit_transform(corpus).toarray())\n",
    "print('BOW Vocabulary', vector.vocabulary_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c53aef50907d0156",
   "metadata": {},
   "source": [
    "## 10.1.3 TF-IDF\n",
    "- TF-IDF(Term Frequency-Inverse Document Frequency)는 백오브워즈의 단점을 보완하기 위해 등장한 방법\n",
    "$$ \\mathrm {TFIDF} (w)=\\mathrm{TF}(w) \\times \\log(N / \\mathrm{DF}(w)) $$\n",
    "- TF(w): 문서에서 단어 w의 빈도\n",
    "- DF(w): 단어 w가 등장한 문서의 수\n",
    "- N: 전체 문서의 수\n",
    "\n",
    "| | TF(\"이\") | TF(\"LLM\") | DF(\"이\") | DF(\"LLM\") | TF-IDF(\"이\") | TF-IDF(\"LLM\") |\n",
    "|---|---------|-----------|---------|-----------|-------------|--------------|\n",
    "| 경제 기사 1 | 10      | 0       | 4       | 2         | 0           | 0            |\n",
    "| 경제 기사 2 | 8       | 0       | 4       | 2         | 0           | 0            |\n",
    "| IT 기사 1 | 5       | 4       | 4       | 2         | 0           | 4 * log(4/2) |\n",
    "| IT 기사 2 | 9       | 2         | 4       | 2         | 0           | 2 * log(4/2) |\n",
    "\n",
    "- 조사 '이'는 모든 문서에 등장하기 때문에 TF-IDF(\"이\")는 0이 된다. (중요도가 없다.)\n",
    "- 백오브워즈의 문제를 성공적으로 보완하면서 오랫동안 활발히 사용\n",
    "    - 이후에 설명하는 BM25(Best Matching 25)와 같은 TF-IDF의 변형 방식이 현재까지도 가장 보편적인 연관도 점수 계산 방식으로 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d915a2f6ce63e23",
   "metadata": {},
   "source": [
    "## 10.1.4 워드투벡\n",
    "- 워드투벡(Word2Vec)은 단어가 `함께 등장하는 빈도` 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법\n",
    "  - \"AI\"는 \"ML\" 또는 \"머신러닝\"\n",
    "  - \"한강\"은 \"라면\"이나 \"자전거\"\n",
    "\n",
    "---\n",
    "\n",
    "- 특정 단어 주변에 어떤 단어가 있는지 예측하는 모델을 만든다면 단어의 의미를 표현한 임베딩을 모델(인공신경망)이 생성할 수 있지 않을까 하는 가정\n",
    "  - 주변 단어로 중간 단어를 예측하는 방식(CBOW, Continuous Bagof Words)\n",
    "  - 중간 단어로 주변 단어를 예측하는 방식(Skip-Gram)\n",
    "\n",
    "![image.png](../resources/Word2Vec-architecture.png)\n",
    "![image.png](../resources/Word2Vec-data.png)\n",
    "\n",
    "---\n",
    "\n",
    "- CBOW는 주변의 단어 정보로 중간에 있을 단어를 예측하는 방식\n",
    "    - t번째 단어를 예측하기 위해 위아래로 2개의 단어 정보를 활용 (t-2, t-1, t+1, t+2)\n",
    "\n",
    "![image.png](../resources/Word2Vec-cbow.png)\n",
    "\n",
    "---\n",
    "\n",
    "- Skip-Gram은 중간 단어로 주변 단어를 예측하는 방식\n",
    "    - t번째 단어를 중심으로 위아래로 2개의 단어를 예측 (t-2, t-1, t+1, t+2)\n",
    "\n",
    "![image.png](../resources/Word2Vec-skipgram.png)\n",
    "![image.png](../resources/Word2Vec-skipgram2.png)\n",
    "\n",
    "---\n",
    "\n",
    "- 벡터간 유사도를 계산하여 단어 사이의 관계/의미를 확인할 수 있다.\n",
    "\n",
    "![image.png](../resources/Word2Vec-relation.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
